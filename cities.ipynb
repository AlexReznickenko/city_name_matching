{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Цель: </b>\n",
    "\n",
    "Сопоставление произвольных гео названий с унифицированными именами geonames для внутреннего использования Карьерным центром.\n",
    "\n",
    "<b>Используемые для решения задачи данные:</b>\n",
    "\n",
    "В рамках данной задачи, были использованы данные, взятые с сайта <a>http://download.geonames.org/export/dump/</a>, а именно файлы alternateNamesV2, cities15000, countryInfo. Заметим, что эти данные использовались именно для построения решения задачи. Заказчик может использовать в практических целях другие данные. Это необходимо учесть в процессе решения задачи. Кроме того, заказчик предоставил файл \n",
    "geo_test.csv для тестирования модели. \n",
    "\n",
    "<b>Задачи:</b>\n",
    "\n",
    "\n",
    "- Создать решение для подбора наиболее подходящих названий с geonames. Например Ереван -> Yerevan. Помимо городов, там могут быть и страны.\n",
    "\n",
    "\n",
    "- На примере РФ и стран наиболее популярных для релокации - Беларусь, Армения, Казахстан, Кыргызстан, Турция, Сербия. Города с населением от 15000 человек (с возможностью масштабирования на сервере заказчика)\n",
    "\n",
    "\n",
    "- Возвращаемые поля geonameid, name, region, country, cosine similarity\n",
    "- формат данных на выходе: список словарей, например [{dict_1}, {dict_2}, …. {dict_n}] где словарь - одна запись с указанными полями\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Дополнительные задачи:</b>\n",
    "\n",
    "\n",
    "- возможность настройки количества выдачи подходящих названий (например в параметрах метода)\n",
    "\n",
    "\n",
    "- коррекция ошибок и опечаток. Например Моченгорск -> Monchegorsk\n",
    "\n",
    "\n",
    "- хранение в PostgreSQL данных geonames\n",
    "\n",
    "\n",
    "- хранение векторизованных промежуточных данных в PostgreSQL\n",
    "\n",
    "\n",
    "- предусмотреть методы для настройки подключения к БД\n",
    "\n",
    "\n",
    "- предусмотреть метод для инициализации класса (первичная векторизация geonames)\n",
    "\n",
    "\n",
    "- предусмотреть методы для добавления векторов новых гео названий\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание проекта и пайплайн решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание исспользуемых данных\n",
    "\n",
    "- cities15000 : таблица, содержащая унифицированные названия городов и их уникальный идентефекатор, а также код страны (например, RU для России) и региона. В рамках задачи используются только те города, которые относятся к перечисленным выше странам.\n",
    "- alternateNamesV2 : таблица, содержащая некоторое количество альтернативных названий для каждого города (названия на разных языках, исторические названия и т.п.), а так же их уникальный идентефекатор. В рамках задачи используются только те города, которые относятся к перечисленным выше странам.\n",
    "- countryInfo : таблица, содержащая названия стран и их коды.\n",
    "- geo_test.csv : таблица, содержащая \"запрос\" и истинное название города. Нужна для тестирования решения.\n",
    "- admin1CodesASCII : таблица, необходимая для извлечения названий регионов.\n",
    "\n",
    "### Описание решения задачи\n",
    "\n",
    "Суть задачи сводится к тому, чтобы сопоставить введённое пользователем слово со словами из таблицы с унифицировннаыми названиями (в нашем случае, cities15000), и выбрать топ-k наиболее близких к нему, где k определяется заказчиком (например, топ-5 или любое другое количество). \n",
    "\n",
    "Для этого было решено предварительно векторизовать слова из спика унифицированных названий, а также введённое пользователем слово и затем искать косинусное сходство между векторами. Далее мы узнаём ближайшие топ-k geoname_id и по ним ищем унифицированные названия городов и другую необходимую информацию. Можно было бы векторизовать все слова из таблицы alternateNamesV2, но даже с учётом ограниченной выборки, поиск по векторам занимал бы большое количество времени (порядка 40-60 секунд для одного предсказания), что слишком много для практического применения.\n",
    "\n",
    "Для создания веторов было решено использовать предобученные нейронные сети Sentence Transformers. Эти нейронные сети уже обучены на большом количестве текстов, и даже без дополнительного обучения, некоторые из них способны решить задачи. Они находятся на сайте <a>https://huggingface.co/sentence-transformers</a>. \n",
    "\n",
    "В рамках решения задачи была выбрана нейросеть <a>https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2</a>. Она показывает неплохие результаты, и кроме того, достаточно \"лёгкая\" для того, чтобы её можно было дообучить с имеющимися мощностями. Модель <a>https://huggingface.co/sentence-transformers/LaBSE</a> показывала лучшие результаты \"из коробки\", то есть без дообучения, однако для того, чтобы её дообучать, необходимы недоступные вычислительные мощносити. В результате, выбранная модель с учётом дообучения победила.\n",
    "\n",
    "Для улучшения показателй было решено дообучить её на выборке из таблицы alternateNamesV2, сопоставив альтернативные и истинные названия. Кроме того, для увеличения эффективности обучения было решено предобработать данные, приведя их к единому виду (нечто вроде лемматизации) и расширить обучающую выборку, добавив в неё также альтернативные названия со случайными опечатками, сгенерированными скриптом. \n",
    "\n",
    "<b>Стадии решения задачи:</b>\n",
    "\n",
    "- Загрузка необходимых данных из датабазы : соединение с postgreSQL базой данных, создания запросов, которые выгрузят необходимую выборку по странам. Обзор данных.\n",
    "\n",
    "- Подготовка данных для обучения : лемматизация городов из alternateNamesV2; сопоставление их с унифицированными названиеми по geonameid;объединение с названиями стран. Данные по странам были также расширены - туда вручную были внесены альтернативные названия (в основном, перевод названий стран на русский язык) и им искуственно присвоен уникальный geonameid; расширение обучающей выборки методом добавления случайных опечаток.\n",
    "\n",
    "- Обучение модели и сохранение обученной модели.\n",
    "\n",
    "- Векторизация городов и стран с помощью обученной модели.\n",
    "\n",
    "- Тестирование получившейся модели.\n",
    "\n",
    "- Итоговая функция, возвращающая требуемый список словарей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доролнительные задачи проекта\n",
    "\n",
    "Помимо решения основной задачи, мы предусмотрели дополнительные пожелания и удобство заказчика. Под этим подразумевается две вещи:\n",
    "\n",
    "<b>Сохранение данных, требующих вычислений</b>, таких как векторезованные слова в отдельную схему базы данных. Таким образом, заказчику не придётся каждый раз векторизовать слова при запуске проекта. Достаточно сделать это один раз и результаты вычислений будут записаны в базу данных, повторное вычисление векторов потребуется произвести только при изменении данных или модели. То же касается и других вещей: обученной модели и предподготовленных данных. Для этого было решено создать дополнительную схему с одной строчкой, где будет указано, какие расчёты уже были сделаны. Более того, эта схема обновляется автоматически (при желании заказчика).\n",
    "\n",
    "<b>Отсутствие необходимости вмешиваться в код программы при изменении данных или модели</b>. Мы учли тот факт, что при реальном применении решения задачи, заказчик может использовать другие данные - например, он может захотеть расширить датасет, или банально могут отличаться названия таблиц и полей. Кроме того, изменение данных может потребовать повторного переобучения модели, создания векторов и так далее, и даже выбора другой модели или параметров обучения. \n",
    "\n",
    "Программа была спректирована таким образом, чтобы пользователю было достаточно изменить значения некоторых константных переменных, чтобы подстроить программу под свои нужды. \n",
    "\n",
    "В файле <b>setup.py</b> находятся все переменные, связанные с базой данных - параметры, необходимые для подключения к БД (пароль, хост и т.п.), а также названия таблиц и использованных полей. Фактически, пользователь может загрузить любые данные с любыми названиями, лишь бы их структура соотвествовала тем, что были предоставлены при решении задачи (точнее, необходимые поля).\n",
    "\n",
    "Переменные, связанные с параметрами обучения модели, а также выбором самой модели, вместе с подробным описанием, находятся сразу после секции с подключением модуля и подключением к БД. \n",
    "\n",
    "<i>Таким образом, для использования данного решения, заказчику достаточно лишь поменять значения некоторых констант. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модулей, подключение к БД и инициализация пользовательских параметров\n",
    "\n",
    "### Подгрузка необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейросеть, используемая для решения задачи\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from sentence_transformers.util import semantic_search\n",
    "from torch.utils.data import DataLoader\n",
    " \n",
    "# Модуль, необходимый для работы с np.Array\n",
    "import numpy as np\n",
    "\n",
    "# Модуль для работы с выгруженными данными в Питоне\n",
    "import pandas as pd\n",
    "\n",
    "# Модули для лемматизации (предобработки) слов\n",
    "import random\n",
    "# Выбираем сид для того, чтобы при повторном запуске все генерируемые псевдослучайные числа не менялись\n",
    "random.seed(42)\n",
    "import unidecode\n",
    "\n",
    "# Модули для подключения к базе данных\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "import psycopg2\n",
    "\n",
    "# \"Декоративный\" модуль для отображения полосы прогресса при вычислениях\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Утилита для перезагрузки пользовательских модулей в случае их изменения\n",
    "import importlib  \n",
    "\n",
    "# Модуль, содержащий константные переменные, связанные с базой данных, такие как параметры для подключения к БД, название таблиц и полей.\n",
    "import setup\n",
    "importlib.reload(setup)\n",
    "from setup import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение к базе данных PostgreSQL\n",
    "\n",
    "Создадим подключение к базе данных для того, чтобы далее  мы могли обращаться к ней и считывать данные или записывать новые. Все параметры подключения находятся в файле <b>setup.py</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = {\n",
    "    'drivername': DRIVERNAME,\n",
    "    'username': USERNAME, \n",
    "    'password': PASSWORD, \n",
    "    'host': HOST,\n",
    "    'port': PORT,\n",
    "    'database': DATABASENAME,\n",
    "    'query': {}\n",
    "}  \n",
    "\n",
    "engine = create_engine(URL(**DATABASE))\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=HOST,\n",
    "    database=DATABASENAME,\n",
    "    user=USERNAME,\n",
    "    password=PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация пользовательских параметров\n",
    "\n",
    "Ниже перечислены константы, которые являются настраиваемыми параметров. То есть, для того, чтобы изменить параметры работы программы, достаточно изменить значения констант. Ниже будут приведены все константы и их подробное описание, а также описание того, в каком случае их стоит менять.\n",
    "Следующая за ними секция также содержит константы, связанные с необходимостью повторных вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБЩЕЕ #\n",
    "\n",
    "# Константа содержащая страны, города которых учитываются. По условию задачи, это \n",
    "# Россия, Беларусь, Армения, Казахстан, Кыргызстан, Турция, Сербия. Если вы хотите добавить страну, просто добавьте её буквенный код.\n",
    "# Страну надо добавлять в том же формате, в котором изначальное значение переменной, то есть добаветь к строке в одиночных кавычках.\n",
    "# Так будет выглядеть переменная, если вы захотите добавить, например, США:\n",
    "# COUNTRIES =\" 'BY', 'AM', 'KZ', 'KG', 'TR', 'RS', 'RU', 'US' \"\n",
    "COUNTRIES =\"'BY', 'AM', 'KZ', 'KG', 'TR', 'RS', 'RU'\"\n",
    "\n",
    "# ПАРАМЕТРЫ ФОРМИРОВАНИЯ ОБУЧАЮЩЕЙ ВЫБОРКИ #\n",
    "\n",
    "# Использование дополнительных альтернативных названий для стран. Данные были сформированы вручную (в основном, посредством перевода).\n",
    "# Если не хотите их использовать, поставьте значение False. \n",
    "MORE_ALTERNATE_COUNTRIES = True\n",
    "\n",
    "# Если вы хотите использовать дополнительные данные из csv файла:\n",
    "ALTERNATE_COUNTRIES_CSV = 'data/more_alternate_names_countries.csv'\n",
    "\n",
    "# Если вы загрузили дополнительные данные по странам в базу, то поставьте этой переменной значение True. Название таблицы в setup.py\n",
    "ALTERNATE_COUNTRIES_FROM_DATABASE = True\n",
    "\n",
    "# Расширение обучающей выборки посредством добавления опечаток. От значения переменной зависит то, сколько раз эта процедура будет \n",
    "# произведена. Т.е. во время каждой итерации проходим по базовому (без опечаток) обучающему датасету, генерируем опечатки и добавляем\n",
    "# в итоговый обучающий датасет. Увеличение этого параметра улучшает качество обучения, но и увеличивает объём обучающей выборки в \n",
    "# практически (так как опечатки могут изредка повторяться и мы потом убираем дубликаты) в (1 + количество итераций) раз! \n",
    "# Это значительно увеличит время обучения. Экспериментальным путём было принято решение остановиться на трёх итерациях, Но вы можете\n",
    "# изменить этот параметр.\n",
    "WARPS = 2\n",
    "\n",
    "# ПАРАМЕТРЫ МОДЕЛИ И ЕЁ ОБУЧЕНИЯ #\n",
    "\n",
    "# Путь к модели, которую собираетесь обучать. Если вы не собираетесь дообучать вашу модель, эта переменная не имеет значения. \n",
    "# Вы можете скачать модели с сайта https://huggingface.co/sentence-transformers, если хотите обучить не ту модель, которая \n",
    "# предложена в решении. \n",
    "# \n",
    "# Чтобы сделать это надо либо склонировать репозиторий в выбранную вами папку и прописать полный путь, как это сделано в проекте.\n",
    "# либо просто прописать, например\n",
    "# BASE_MODEL =  'sentence-transformers/LaBSE'. \n",
    "# Во втором случае модель установится в папку с Питоном по умолчанию (это может занять много времени, если модель большая) \n",
    "# Вы можете указать путь и на уже дообученную модель, чтобы дообучить её дальше.\n",
    "BASE_MODEL =  './models/distiluse-base-multilingual-cased-v2'\n",
    "\n",
    "# Путь к модели, которая будет использоваться непосредственно в решении задачи. Также тот путь, куда будет сохранена обученная модель.\n",
    "# Если вы не хотите дообучать модель, то только ЭТА переменная имеет значения, предыдущая не важна.\n",
    "# Значение переменной может совпадать с предыдущей.\n",
    "FINAL_MODEL = './models/distiluse-base-multilingual-cased-v2-4epochs-CITIESFINDER'\n",
    "\n",
    "# Количество эпох, которые обучается модель. До определённого момента улучшает показатели, но затем может переобучиться, точно предсказать \n",
    "# оптимальное число эпох невозможно, это зависит от модели и данных. Одна эпоха может занять много времени!\n",
    "EPOCHS = 4\n",
    "\n",
    "# Размер \"батча\". Чем больше, тем быстрее обучается модель, но при том она требует больше оперативной памяти. \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ТЕСТИРОВАНИЕ МОДЕЛИ #\n",
    "\n",
    "# Путь к тестовому файлу. Тестовый файл содержит поля 'query' с тестовым запросом и 'name' с истинным названием города. \n",
    "# Названия должны быть именно такими. Разделитель - ';'.\n",
    "TEST_CSV = 'data/geo_test.csv'\n",
    "\n",
    "# Функция тестирования возвращает две метрики: accuracy - процент попадания нужного города в первую строчку вывода модели и\n",
    "# accuracy*k - процент попаданий нужного города в список из k строк. Этот параметр отвечает за величину k\n",
    "\n",
    "TEST_K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры записи результатов вычислений в датабазу и повторных вычислений\n",
    "\n",
    "Некоторые шаги выполнения программы требуют вычислений и времени, а именно: обучение модели и преобразование слов в векторы. Было решено записать результаты вычислений в отдельные таблицы базы данных (модель сохраняется в папке проекта). Для того, чтобы программа могла понять, нужно ли делать вычисления или они уже были выполнены, было решено добавить вспомогательную таблицу данных в БД, содержащую одну строку, в которой указывается были ли сделаны те или иные шаги. Затем программа считывает эту таблицу и смотрит на то, какие вычисления необходимо выполнить. \n",
    "\n",
    "Таблица будет содержать нули в тех полях, где вычисления ещё не сделаны. Таким образом, если в таблице ноль - то программа делает вычисления, если один - то нет. По желанию пользователя таблица может автоматически обновляться после вычислений. \n",
    "\n",
    "Если после произведённых вычислений требуется сделать их повторно (например, обучить новую модель), то у пользователя есть два пути:\n",
    "1. Обновить таблицу прямо в коде программы, вручную выставив необходимые опции и присвоив соответствующему параметру значение True (описание опций и параметра ниже)\n",
    "2. Обновить таблицу средствами управления базой данных. \n",
    "\n",
    "Второй вариант необходим так как позволит автоматизировать обновление таблицы, например, когда те или иные данные обновляются. Скажем, инженер данных со стороны заказчика мог бы написать скрипт, который ставит всем параметрам в таблице значения равные нулю когда дополняется таблица, содержащая альтернативные названия городов на определённое количество данных. Тогда при запуске скрипта модель заново обучится и векторы будут завово созданы, это не придётся каждый раз контролировать вручную. Однако, обновить таблицу можно и вручную.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели. Нужно если вы хотите обучить новую модель или добучить старую. Если значение равно 1, значит модель уже обучена и не будет\n",
    "# повторно обучаться. Потому если вы хотите обучить модель заново - значение должно быть равно нулю, а не наоборот, как могло показаться!\n",
    "\n",
    "FIT_MODEL = 0\n",
    "\n",
    "# Векторизация слов из поисковой выборки. Повторная векторизация необходима если вы заново обучили модель или если вы изменили выборку\n",
    "# по которой модель ищет истинные названия городов (в нашем случае - cities1500)\n",
    "VECTORIZATION = 0\n",
    "\n",
    "# Обновление таблицы с опциями. Если значение равно True, то программа при каждом запуске будет переписывать таблицу с опциями в соответствии \n",
    "# со значениями констант, используемых выше. Если False, программа не будет этого делать, соответственно тогда значения констант не важны. \n",
    "REFRESH_OPTIONS = True\n",
    "\n",
    "# После завершения того или иного этапа обновления, программа запомнит это и автоматически добавит в таблицу с опциями значение 1 в \n",
    "# соответствующем поле. Если вы не хотите, чтобы она это делала, поставьте константе значение False. \n",
    "AUTO_UPDATE_OPTIONS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем код, который создаст и считает таблицу с опциями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица с опциями обновлена\n"
     ]
    }
   ],
   "source": [
    "# Создадим датафрейм с соответствующими названиями полей и строчкой, заполненной нулями. Он нужен на тот случай, если таблица ещё \n",
    "# не была создана вовсе или вы хотите обновить таблицу в программе. \n",
    "update_database = pd.DataFrame( columns = ['fit_model', 'vectorization'])\n",
    "\n",
    "if REFRESH_OPTIONS:\n",
    "# Если мы хотим менять опции вручную, то присваиваем соответствующим столбцам датафрейма соответствующие значения.\n",
    "    update_database.loc[0] = [FIT_MODEL, VECTORIZATION]\n",
    "else: \n",
    "# Если опции менять не нужно, присваиваем все нули. Это нужно исключительно на тот случай, если таблица вовсе не была создана.\n",
    "    update_database.loc[0] = [0, 0]\n",
    "\n",
    "if REFRESH_OPTIONS:\n",
    "# Если хотим обновить опции вручную, просто переписываем таблицу с опциями, ставя значение if_exists='replace'.\n",
    "     update_database.to_sql(OPTIONS, engine, if_exists='replace', index=False)\n",
    "     print('Таблица с опциями обновлена')\n",
    "# Иначе проверяем, создана ли наша таблица. Для этого попробуем обновить её с параметром if_exists='fail'. Если таблица существует,\n",
    "# это вызовет исключение, но мы используем try-except, так что всё в порядке.\n",
    "else:\n",
    "    try:\n",
    "         update_database.to_sql('model_options_table', engine, if_exists='fail', index=False)\n",
    "         print('Таблица с опциями создана')\n",
    "    except:\n",
    "        print('Таблица с опциями уже существует. Чтобы обновить - присвойте константе REFRESH_OPTIONS значение True')\n",
    "\n",
    "# Считаем таблицу с опциями\n",
    "\n",
    "query = f'SELECT * FROM \"{OPTIONS}\"'\n",
    "options = pd.read_sql_query(query, con=engine)\n",
    "\n",
    "# Запишем считанные опции в отдельные переменные, так как их немного и объединять их в список или словарь не имеет особого смысла\n",
    "FIT_MODEL_OPTION = options.iloc[0]['fit_model']\n",
    "VECTORIZATION_OPTION = options.iloc[0]['vectorization']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы инициализировали пользовательские переменные и опции. Благодаря этому, пользователь сможет настроить программу под себя не вмешиваясь в основной код. Это необходимо также потому, что скорее всего, заказчик будет использовать более полные данные чем те, что он дал для проектирования решения задачи, соответственно, ему придётся обновить вектора и, возможно, заново обучить модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и обзор данных\n",
    "\n",
    "Для решения задачи нам необходимы следующие данные:\n",
    "\n",
    "- cities15000 : таблица, содержащая унифицированные названия городов. Основная таблица для поиска данных.\n",
    "- alternateNamesV2 : таблица, содержащая некоторое количество альтернативных названий для каждого города (названия на разных языках, исторические названия и т.п.).\n",
    "- countryInfo : таблица, содержащая названия стран и их коды.\n",
    "- admin1CodesASCII : таблица, необходимая для извлечения названий регионов.\n",
    "- geo_test.csv : таблица, содержащая \"запрос\" и истинное название города. Нужна для тестирования решения.\n",
    "\n",
    "Сначала загрузим части датасетов и посмотрим на них. Мы будем делать все запросы здесь и далее с использованием констант. То есть, переменная с текстом запроса будет содержать не строку вида <code>'SELECT * FROM \"cities15000\" LIMIT 3'</code>, а она будет генерироваться выражением вида <code>'SELECT * FROM \"' + CITIES + '\" LIMIT 3'</code>, чтобы пользователь мог работать с программой если у него другие названия таблиц. Например, если он использует cities500 вместо cities15000, ему достаточно просто в файле <b>setup.py</b> поменять значение константы  CITIES на соотвествующее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>name</th>\n",
       "      <th>asciiname</th>\n",
       "      <th>alternatenames</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>feature_class</th>\n",
       "      <th>feature_code</th>\n",
       "      <th>country_code</th>\n",
       "      <th>cc2</th>\n",
       "      <th>admin1_code</th>\n",
       "      <th>admin2_code</th>\n",
       "      <th>admin3_code</th>\n",
       "      <th>admin4_code</th>\n",
       "      <th>population</th>\n",
       "      <th>elevation</th>\n",
       "      <th>dem</th>\n",
       "      <th>timezone</th>\n",
       "      <th>modification_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3040051</td>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>Ehskal'des-Ehndzhordani,Escaldes,Escaldes-Engo...</td>\n",
       "      <td>42.50729</td>\n",
       "      <td>1.53414</td>\n",
       "      <td>P</td>\n",
       "      <td>PPLA</td>\n",
       "      <td>AD</td>\n",
       "      <td>None</td>\n",
       "      <td>08</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15853</td>\n",
       "      <td>None</td>\n",
       "      <td>1033</td>\n",
       "      <td>Europe/Andorra</td>\n",
       "      <td>2008-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3041563</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>ALV,Ando-la-Vyey,Andora,Andora la Vela,Andora ...</td>\n",
       "      <td>42.50779</td>\n",
       "      <td>1.52109</td>\n",
       "      <td>P</td>\n",
       "      <td>PPLC</td>\n",
       "      <td>AD</td>\n",
       "      <td>None</td>\n",
       "      <td>07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20430</td>\n",
       "      <td>None</td>\n",
       "      <td>1037</td>\n",
       "      <td>Europe/Andorra</td>\n",
       "      <td>2020-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>290594</td>\n",
       "      <td>Umm Al Quwain City</td>\n",
       "      <td>Umm Al Quwain City</td>\n",
       "      <td>Oumm al Qaiwain,Oumm al Qaïwaïn,Um al Kawain,U...</td>\n",
       "      <td>25.56473</td>\n",
       "      <td>55.55517</td>\n",
       "      <td>P</td>\n",
       "      <td>PPLA</td>\n",
       "      <td>AE</td>\n",
       "      <td>None</td>\n",
       "      <td>07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>62747</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Asia/Dubai</td>\n",
       "      <td>2019-10-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geoname_id                name           asciiname  \\\n",
       "0     3040051        les Escaldes        les Escaldes   \n",
       "1     3041563    Andorra la Vella    Andorra la Vella   \n",
       "2      290594  Umm Al Quwain City  Umm Al Quwain City   \n",
       "\n",
       "                                      alternatenames  latitude  longitude  \\\n",
       "0  Ehskal'des-Ehndzhordani,Escaldes,Escaldes-Engo...  42.50729    1.53414   \n",
       "1  ALV,Ando-la-Vyey,Andora,Andora la Vela,Andora ...  42.50779    1.52109   \n",
       "2  Oumm al Qaiwain,Oumm al Qaïwaïn,Um al Kawain,U...  25.56473   55.55517   \n",
       "\n",
       "  feature_class feature_code country_code   cc2 admin1_code admin2_code  \\\n",
       "0             P         PPLA           AD  None          08        None   \n",
       "1             P         PPLC           AD  None          07        None   \n",
       "2             P         PPLA           AE  None          07        None   \n",
       "\n",
       "  admin3_code admin4_code  population elevation   dem        timezone  \\\n",
       "0        None        None       15853      None  1033  Europe/Andorra   \n",
       "1        None        None       20430      None  1037  Europe/Andorra   \n",
       "2        None        None       62747      None     2      Asia/Dubai   \n",
       "\n",
       "  modification_date  \n",
       "0        2008-10-15  \n",
       "1        2020-03-03  \n",
       "2        2019-10-24  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'SELECT * FROM \"{CITIES}\" LIMIT 3'\n",
    "pd.read_sql_query(query, con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересуют поля geoname_id, name, asciiname, admin1_code и country_code\tдля создания выборки по интересующим нас странам. Заметим, что здесь присутствует столбец alternatenames, который содержит альтернативные имена, записанные в виде строчки. Но у нас имеется таблица с альтернативными именами, и она удобней для обучения данных.\n",
    "\n",
    "#### alternateNamesV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternatenameid</th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>isolanguage</th>\n",
       "      <th>alternate_name</th>\n",
       "      <th>ispreferredname</th>\n",
       "      <th>isshortName</th>\n",
       "      <th>iscolloquial</th>\n",
       "      <th>ishistoric</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1284819</td>\n",
       "      <td>2994701</td>\n",
       "      <td>None</td>\n",
       "      <td>Roc Mélé</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1284820</td>\n",
       "      <td>2994701</td>\n",
       "      <td>None</td>\n",
       "      <td>Roc Meler</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4285256</td>\n",
       "      <td>3007683</td>\n",
       "      <td>None</td>\n",
       "      <td>Pic des Langounelles</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alternatenameid  geoname_id isolanguage        alternate_name  \\\n",
       "0          1284819     2994701        None              Roc Mélé   \n",
       "1          1284820     2994701        None             Roc Meler   \n",
       "2          4285256     3007683        None  Pic des Langounelles   \n",
       "\n",
       "  ispreferredname isshortName iscolloquial ishistoric  from    to  \n",
       "0            None        None         None       None  None  None  \n",
       "1            None        None         None       None  None  None  \n",
       "2            None        None         None       None  None  None  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'SELECT * FROM \"{ALTERNATE_NAMES}\" LIMIT 3'\n",
    "pd.read_sql_query(query, con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, эта таблица огромная, но нас интересует ограниченная выборка из стран. В ней нас интересуют исключительно столбцы geoname_id и alternate_name. Таблица нужна только для обучения модели, иначе её нет смысл загружать.\n",
    "\n",
    "#### countryInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso</th>\n",
       "      <th>iso3</th>\n",
       "      <th>isonumeric</th>\n",
       "      <th>fips</th>\n",
       "      <th>country</th>\n",
       "      <th>capital</th>\n",
       "      <th>area</th>\n",
       "      <th>population</th>\n",
       "      <th>continent</th>\n",
       "      <th>tld</th>\n",
       "      <th>currencycode</th>\n",
       "      <th>currencyname</th>\n",
       "      <th>phone</th>\n",
       "      <th>postal_code_format</th>\n",
       "      <th>postal_code _regex</th>\n",
       "      <th>languages</th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>neighbours</th>\n",
       "      <th>equivalentfipscode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AD</td>\n",
       "      <td>AND</td>\n",
       "      <td>20</td>\n",
       "      <td>AN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>468.0</td>\n",
       "      <td>77006</td>\n",
       "      <td>EU</td>\n",
       "      <td>.ad</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Euro</td>\n",
       "      <td>376</td>\n",
       "      <td>AD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>ARE</td>\n",
       "      <td>784</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>82880.0</td>\n",
       "      <td>9630959</td>\n",
       "      <td>AS</td>\n",
       "      <td>.ae</td>\n",
       "      <td>AED</td>\n",
       "      <td>Dirham</td>\n",
       "      <td>971</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ar-AE,fa,en,hi,ur</td>\n",
       "      <td>290557.0</td>\n",
       "      <td>SA,OM</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>4</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>647500.0</td>\n",
       "      <td>37172386</td>\n",
       "      <td>AS</td>\n",
       "      <td>.af</td>\n",
       "      <td>AFN</td>\n",
       "      <td>Afghani</td>\n",
       "      <td>93</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>fa-AF,ps,uz-AF,tk</td>\n",
       "      <td>1149361.0</td>\n",
       "      <td>TM,CN,IR,TJ,PK,UZ</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iso iso3  isonumeric fips               country           capital      area  \\\n",
       "0  AD  AND          20   AN               Andorra  Andorra la Vella     468.0   \n",
       "1  AE  ARE         784   AE  United Arab Emirates         Abu Dhabi   82880.0   \n",
       "2  AF  AFG           4   AF           Afghanistan             Kabul  647500.0   \n",
       "\n",
       "   population continent  tld currencycode currencyname phone  \\\n",
       "0       77006        EU  .ad          EUR         Euro   376   \n",
       "1     9630959        AS  .ae          AED       Dirham   971   \n",
       "2    37172386        AS  .af          AFN      Afghani    93   \n",
       "\n",
       "  postal_code_format postal_code _regex          languages  geoname_id  \\\n",
       "0                 AD               None               None         NaN   \n",
       "1               None               None  ar-AE,fa,en,hi,ur    290557.0   \n",
       "2               None               None  fa-AF,ps,uz-AF,tk   1149361.0   \n",
       "\n",
       "          neighbours equivalentfipscode  \n",
       "0               None               None  \n",
       "1              SA,OM               None  \n",
       "2  TM,CN,IR,TJ,PK,UZ               None  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'SELECT * FROM \"{ COUNTRY_INFO}\"'\n",
    "df = pd.read_sql_query(query, con=engine)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересуют сами страны (столбец country), а также geoname_id. Эта таблица нам нужна, так как помимо городов, пользователи могут в качестве локации указывать свою страну. Однако, мы сразу видим пропуск в geoname_id. Также в таблице содержится столбец iso, откуда можно было взять коды стран для создания выборки, однако удобней было посмотреть эти коды в интернете, чем искать в таблице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 158)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на общую длину таблицы со странами и количество пропусков в geoname_id.\n",
    "len(df[GEONAME_ID_COLUMN]), df[GEONAME_ID_COLUMN].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что у большинства стран пропуски в колонке geoname_id, что нас не устраивает. Было решено искуственно присвоить им идентефикаторы, что сделаем ниже.\n",
    "\n",
    "#### admin1CodesASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "      <th>name_ascii</th>\n",
       "      <th>geoname_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AD.06</td>\n",
       "      <td>Sant Julià de Loria</td>\n",
       "      <td>Sant Julia de Loria</td>\n",
       "      <td>3039162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AD.05</td>\n",
       "      <td>Ordino</td>\n",
       "      <td>Ordino</td>\n",
       "      <td>3039676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AD.04</td>\n",
       "      <td>La Massana</td>\n",
       "      <td>La Massana</td>\n",
       "      <td>3040131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                 name           name_ascii  geoname_id\n",
       "0  AD.06  Sant Julià de Loria  Sant Julia de Loria     3039162\n",
       "1  AD.05               Ordino               Ordino     3039676\n",
       "2  AD.04           La Massana           La Massana     3040131"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'SELECT * FROM \"{ADMIN_CODES}\" LIMIT 3'\n",
    "pd.read_sql_query(query, con=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересует столбец code и name, чтобы извлечь название региона конкретного города, которое нам также нужно по условию задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных для дальнейшего использования\n",
    "\n",
    "Мы сделали краткий обзор данных и теперь нам нужно выгрузить конкретно необходимые нам данные, а также подготовить их для того, чтобы использовать нашу модель.\n",
    "\n",
    "Таблицу <b>Загрузим admin1CodesASCII</b> пока загружать нет смысла, так как она понадобится исключительно в самом конце для финальной функции.\n",
    "\n",
    "#### Загрузка таблицы со странами\n",
    "Загрузим для начала страны. Мы видели, что в столюце geoname_id очень много пропусков. Потому мы заполним этот столбец отрицательными значениями, соотвествующие индексу записи в таблице если там пропуск и просто отрицательными значениями, если geoname_id определён.\n",
    "\n",
    "Это нужно для нашей функции поиска ближайших названий локации, чтобы понять относится ли локация к городам или стране."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country       0\n",
       "geoname_id    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>geoname_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>-290557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-1149361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>-3576396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>-3573511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                country  geoname_id\n",
       "0               Andorra           0\n",
       "1  United Arab Emirates     -290557\n",
       "2           Afghanistan    -1149361\n",
       "3   Antigua and Barbuda    -3576396\n",
       "4              Anguilla    -3573511"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Считываем таблицу со странами. Мы используем константы вместо строк в названиях таблицы и столбцов, чтобы пользователь мог использовать\n",
    "# другие имена и ему не пришлось бы переписывать код.\n",
    "\n",
    "# Сама функция нам ещё пригодится\n",
    "def make_countries(save_country_code = False):\n",
    "    if save_country_code:\n",
    "        query = f'SELECT\"{COUNTRY_COLUMN}\", \"{GEONAME_ID_COLUMN}\", \"{ISO_COLUMN}\" FROM \"{COUNTRY_INFO}\"'\n",
    "    else:\n",
    "        query = f'SELECT\"{COUNTRY_COLUMN}\", \"{GEONAME_ID_COLUMN}\" FROM \"{COUNTRY_INFO}\"'\n",
    "    df_countries = pd.read_sql_query(query, con=engine)\n",
    "    # Нам надо заполнить пропуски и заменить существующие geo_id на отрицательные значения для удобства написании финальной функции.\n",
    "    # Напишем функцию, которую потом используем в apply\n",
    "    def set_geoname_id(country_index, country_id):\n",
    "    # Определяем пропуски\n",
    "        if country_id != country_id:\n",
    "            # Если пропуск, то присваиваем значение равное минус индексу записи в таблице. \n",
    "            return -country_index\n",
    "        else:\n",
    "            # Если нет пропуска, то просто возвращаем имеющееся значение, умноженное на минус единицу\n",
    "            return -country_id\n",
    "    \n",
    "# Теперь надо применить нашу функцию. Для этого надо во-первых извлечь индексы, во-вторых использовать лямбда функцию, чтобы наша функция\n",
    "# могла получить значения двух разных столбцов в качестве аргументов.    \n",
    "    df_countries[GEONAME_ID_COLUMN] = df_countries.reset_index(inplace= False )[['index', GEONAME_ID_COLUMN]].\\\n",
    "    apply(lambda row: set_geoname_id(row['index'], row[GEONAME_ID_COLUMN]), axis = 1)\n",
    "\n",
    "# Значения geonames_id были float, так как изначально там присутствовали пропуски, переведём в int.\n",
    "    df_countries[GEONAME_ID_COLUMN] = df_countries[GEONAME_ID_COLUMN].astype(int)\n",
    "# Посмотрим на результат\n",
    "    return df_countries\n",
    "\n",
    "df_countries = make_countries()\n",
    "display(df_countries.isna().sum())\n",
    "df_countries.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы успешно загрузили таблицу со странами и справились с пропусками. \n",
    "\n",
    "#### Загрузка таблицы cities15000\n",
    "\n",
    "Теперь нам нужно загрузить данные из таблицы cities15000, но только те, что соответствуют нужным по условиям задачи странам. Это Россия, Беларусь, Армения, Казахстан, Кыргызстан, Турция и Сербия. Они уже записаны в константе COUNTRIES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name           0\n",
       " geoname_id     0\n",
       " admin1_code    0\n",
       " dtype: int64,\n",
       " (1711, 19))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Формируем запрос, который выберет только те города, в которых значение country_code равно коду одной из стран, которые мы используем.\n",
    "query = 'SELECT * FROM \"' + CITIES  + '\" WHERE\"' +COUNTRY_CODE_COLUMN + '\"'+ ' IN (' + COUNTRIES + ')'\n",
    "df_cities = pd.read_sql_query(query, con=engine)\n",
    "\n",
    "# Если честно, я изначально во всём коде использовал именно колонку с именем, только потом сообразил, \n",
    "# что целесообразней использовать asciiname. Чтобы не перелопачивать весь код, я просто присвоил колонке с именами нужное мне значение\n",
    "df_cities[NAME_COLUMN] = df_cities[ASCII_NAME_COLUMN]\n",
    "\n",
    "# Посмотрим, есть ли пропуски в интересующих нас столбцах. Столбец country_code проверять не нужно, так как даже если там были пропуски,\n",
    "# они бы не попали в наш запрос.\n",
    "df_cities[[NAME_COLUMN, GEONAME_ID_COLUMN, ADMIN_CODES_COLUMN]].isna().sum(), df_cities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропусков нет. Создадим новую переменную df_cities_emb, которую позже будем использовать для поиска косинусных расстояний. В переменной будут храниться те же данные, что в df_cities, но поля только с именем и geoname_id. Позже в неё добавим страны. После эту переменную превратим в векторы и именно от неё зависит, среди каких названий модель будет искать сходства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geoname_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kapan</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  geoname_id\n",
       "0  Kapan      174875"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities_emb = df_cities[[NAME_COLUMN,GEONAME_ID_COLUMN]].copy()\n",
    "df_cities_emb.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Загрузка альтернативных названий городов\n",
    "\n",
    "Нам осталось загрузить альтернативные имена городов. Нам нужны альтернативные названия только тех городов, которые находятся в списке интересующих нас стран. Но в самой таблице с альтернативными названиями нет столбца с кодом страны, есть только столбец с geoname_id, а потому нам нужно извлечь все geoname_id из списка наших городов и использовать их как фильтр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternate_name</th>\n",
       "      <th>geoname_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qafan</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kapan</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kapan</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kapan</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کاپان</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Կապան</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Капан</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Kapan</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://ru.wikipedia.org/wiki/%D0%9A%D0%B0%D0%...</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Капан</td>\n",
       "      <td>174875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      alternate_name  geoname_id\n",
       "0                                              Qafan      174875\n",
       "1                                              Kapan      174875\n",
       "2                                              Kapan      174875\n",
       "3                                              Kapan      174875\n",
       "4                                              کاپان      174875\n",
       "5                                              Կապան      174875\n",
       "6                                              Капан      174875\n",
       "7                https://en.wikipedia.org/wiki/Kapan      174875\n",
       "8  https://ru.wikipedia.org/wiki/%D0%9A%D0%B0%D0%...      174875\n",
       "9                                              Капан      174875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "query = (\n",
    "    f'SELECT \"{ALTERNATE_NAME_COLUMN }\",\"{GEONAME_ID_COLUMN}\" FROM \"{ALTERNATE_NAMES}\" '\n",
    "    f'WHERE \"{GEONAME_ID_COLUMN}\" IN ( '\n",
    "    f'SELECT \"{GEONAME_ID_COLUMN}\" FROM \"{CITIES}\" '\n",
    "    f'WHERE \"{COUNTRY_CODE_COLUMN}\" IN ({COUNTRIES}) ) '\n",
    ")\n",
    "df_alternate_names = pd.read_sql_query(query, con=engine)\n",
    "display(df_alternate_names.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативные названия загрузились. Мы видим, что здесь присутствуют названия на разных языках, в том числе с нестандартной раскладкой. Мы напишем функцию, которая стандартизирует слова, переводя раскладку на один язык.\n",
    "\n",
    " Кроме того, здесь есть альтернативные названия вроде \"https://en.wikipedia.org/wiki/Kapan\", что явно лишнее. Это необходимо будет удалить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление стран к обучающей выборки и выборки для создания векторов\n",
    "\n",
    "Мы хотим, чтобы наша модель могла искать не только названия городов, но и стран. Для этого нам надо соединить страны с обучающей выборкой (если создаём) и df_cities_emb, чтобы модель могла искать страны. \n",
    "\n",
    "Также нами была вручную создана дополниьтельная таблица с альтернативными названиями стран. В основном, туда добавлены переведённые с помощью Яндекс Переводчика на русский язык названия стран, но также были добавлены и другие названия для некоторых стран, например, для России были добавлены такие названия, как РФ и Российская Федерация.\n",
    "\n",
    "Альтернативные названия будут использоваться только в обучающей выборке, в выборке для поиска похожих названий будут только унифицированные названия стран. Структура альтернативных названий такова : она имеет только две колонки - country и geoname_id. В колонке country названия стран (оригинальные и альтернативные), в колонке geoname_id - полученные ранее geoname_id (они могут повторяться). Если вы решили использовать эти данные, они добавятся в данные для обучения вместо обычных стран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем, хотите ли вы использовать дополнительные альтернативные названия стран для обучения\n",
    "if MORE_ALTERNATE_COUNTRIES:\n",
    "# Проверяем откуда вы предпочитаете взять альтернативные названия стран - из Базы Данных или csv-файла\n",
    "    if ALTERNATE_COUNTRIES_FROM_DATABASE:\n",
    "        # Формируем запрос, если из БД\n",
    "        query = f'SELECT * FROM \"{ALTERNATE_COUNTRIES}\"'\n",
    "        df_more_alternate_countries = pd.read_sql_query(query, con=engine)\n",
    "    else:\n",
    "        # Считываем из csv-файла если хотите брать из него\n",
    "        df_more_alternate_countries = pd.read_csv(ALTERNATE_COUNTRIES_CSV, sep = ',')\n",
    "\n",
    "# Переименовываем колонку country в alternate_name, чтобы затем объединить с таблицей с альтернативными именами.\n",
    "    df_more_alternate_countries = df_more_alternate_countries.rename(columns = {COUNTRY_COLUMN:ALTERNATE_NAME_COLUMN})\n",
    "# Объединяем с таблицей с альтернативными именами и получаем новую таблицу\n",
    "    df_alter = pd.concat([df_more_alternate_countries,df_alternate_names])\n",
    " \n",
    "else:\n",
    "# Если мы не хотим использовать дополнительные названия стран, то \n",
    "# объелиняем выборку с обычными названиями стран\n",
    "    df_alter = pd.concat([df_countries.rename(columns = {COUNTRY_COLUMN:ALTERNATE_NAME_COLUMN}) ,df_alternate_names])\n",
    "    \n",
    "\n",
    "# В любом случае, объединяем df_cities_emb с обычными названиями странам  \n",
    "df_cities_emb = pd.concat([df_countries.rename(columns = {COUNTRY_COLUMN:NAME_COLUMN}),df_cities_emb])\n",
    "\n",
    "# Старая переменная нам больше не понадобится, потому удалим её \n",
    "del df_alternate_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы успешно объединили обучющую и выборку для векторизации с названиями стран."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка обучающих данных\n",
    "\n",
    "Для обучения мы сформируем выборку, состоящую из двух столбцов - истинные имена и альтернативные имена. Сами модели <b>Sentence-Transformers</b> не ищут похожие слова, они преобразуют слова в векторы. Затем уже с помощью другой функции мы ищем косинусное сходство между векторами и находим самые похожие \"по смыслу\" слова (или предложения, но в нашем случае именно слова). \n",
    "\n",
    "Модель получает на вход пару слов и метку того, на сколько они должны быть похожи. Но поскольку мы обучаем на парах \"истинное имя - альтернативное имя\", метка всегда будет 1 (то есть слова одинаковы по смыслу). У нас получается выборка где все данные имеют одинаковую метку, но модель <b>Sentence-Transformers</b> позволяет обучаться на таких данных, то есть негативные примеры и не нужны.\n",
    "\n",
    "Но перед тем как сформируем обучающие данные, напишем две функции для обработки слов, которые увеличат эффективность.\n",
    "\n",
    "#### Унификация слов\n",
    "\n",
    "Сначала напишем функцию для лемматизации, или, вернее будет сказать, унификации слов, то есть приведём все слова к одному виду - в нижнем регистре, написаны латиницей и без дополнительных символов и пробелов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kariba\n",
      "kryb\n",
      "khrybzmbbwy\n",
      "qialiba\n",
      "kariba\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функция принимает \"сырое\" слово и возвращает обработанное.\n",
    "def prepare_word(word):\n",
    "\n",
    "# Определим символы, которые мы хотим удалять из слов.\n",
    "    bad_symbols = '''\n",
    "    .,-'!`_ \n",
    "\n",
    "    '''\n",
    "# С помощью модуля unidecode переводим любой язык в латинкский алфавит\n",
    "    clear_word = unidecode.unidecode(word)\n",
    "\n",
    "# Удаляем все лишние символы\n",
    "    clear_word = ''.join([i for i in clear_word if i not in bad_symbols])\n",
    "\n",
    "# Приводим слово к нижнему регистру\n",
    "    clear_word = clear_word.lower()\n",
    "    return clear_word\n",
    "\n",
    "\n",
    "# Проверим работу функции. Список взят из альтернативных имён, он специально подобран так, \n",
    "# чтобы тут встречались слова на нестандартных языках\n",
    "[print (prepare_word(i))for i in ['Кариба','كاريبا ','کاریبا، زمبابوے','卡里巴','카리바']]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, функция справилась. Надо учесть что и само звучание слова \"Кариба\" на других языках может в корне отличаться от привычного нам. Да, получившиеся слова отличаются от kariba, но видно, что это не случайные наборы символов. Таким образом, функция справляется с разными языками.\n",
    "\n",
    "#### Аугментация данных\n",
    "\n",
    "Теперь создадим функцию для аугментации данных - то есть синтетического создания новых данных на основе уже имеющихся. В данном случае, мы будем добавлять в слова случайные опечатки. Это как раз соотвествует одной из задач - справляться с опечатками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uskva'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Объявляем функцию\n",
    "\n",
    "def warp(\n",
    "        #Слово, которое мы будем менять \n",
    "        word, \n",
    "        # Определяем минимальную длину слова, в которое будем добавлять опечатки, чтобы не добавлять опечатки в такие слова\n",
    "        # как \"Спб\" или \"РФ\". \n",
    "        min_word_length = 4,\n",
    "        # Количество опечаток, которые мы добавляем к слову\n",
    "        warps = 1,\n",
    "        # Тип опечатки которую мы хотим добавить. Значения могут быть следующими:\n",
    "        # 'replace' - заменить случайный символ из слова на случайный символ\n",
    "        # 'add' - добавить случайный символ в случайное место слова\n",
    "        # 'remove' - удалить случайный символ\n",
    "        # 'shuffle' - поменять местами два соседних символа\n",
    "        # 'random' - тип опечатки выбирается случайным образом из всех вышеперечисленных методов (это мы и будем использовать)\n",
    "        char_warp = 'random'):\n",
    "    \n",
    "    # Так как мы имеем дело с английским алфавитом, применям унификацию к слову\n",
    "    word = prepare_word(word)\n",
    "    # Проверяем длину слова. Если меньше минимально-допустимой, то возвращаем изначальное слово (то есть ничего не делаем с ним)\n",
    "    if len(word) < min_word_length:\n",
    "        return word\n",
    "    \n",
    "    # Добавляем опечатки\n",
    "    while warps > 0:\n",
    "        # Определяем переменную, которая принимает значение случайной буквы из английского алфавита\n",
    "        random_char = random.choice(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "        # Если char_warp = random то выберем случайный метод опечатки, иначе тот, который мы указали. Запишем его в новую переменную\n",
    "        # чтобы при повторной итерации метод снова выбирался случайно (если мы установили соответствующее значение)\n",
    "        if char_warp == 'random':\n",
    "            char_warp_select = random.choice(['replace', 'add', 'remove', 'shuffle'])\n",
    "        else:\n",
    "            char_warp_select = char_warp\n",
    "        \n",
    "        # Перемешиваем ближайшие символы\n",
    "        if char_warp_select == 'shuffle':\n",
    "            choice_char = random.randint(0, len(word) - 2)\n",
    "            word = list(word)\n",
    "            word[choice_char], word[choice_char + 1] = word[choice_char + 1], word[choice_char]\n",
    "            word = \"\".join(word)\n",
    "\n",
    "        # Добавляем случайный символ в случайное место\n",
    "        if char_warp_select == 'add':\n",
    "            choice_char = random.randint(0, len(word) - 1)\n",
    "            word = word[:choice_char] + random_char + word[choice_char:]\n",
    "\n",
    "        # Удаляем случайный символ\n",
    "        if char_warp_select == 'remove':\n",
    "            choice_char = random.randint(1, len(word))\n",
    "            word = word[:choice_char - 1] + word[choice_char:]\n",
    "\n",
    "        # Меняем случайный символ на другой случайный символ\n",
    "        if char_warp_select == 'replace':\n",
    "            choice_char = random.randint(0, len(word) - 1)\n",
    "            word = list(word)\n",
    "            word[choice_char] = random_char\n",
    "            word = \"\".join(word)\n",
    "\n",
    "        # Уменьшаем значение переменной чтобы выйти из цикла\n",
    "        warps -= 1\n",
    "    return(word)\n",
    "\n",
    "# Проверим работу функции\n",
    "warp('Москва', warps = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы проверили работу функции на слове \"Москва\", для наглаядности используя две опечатки. На практике мы остановились на том, что для аугментации данных используем только одну опечатку - слишком сильно искажать данные тоже не стоит, иначе модель может работать неадекватно.\n",
    "\n",
    "В данном случае, у нас получилось слово \"uskva\" - функция удалила первую букву и заменила вторую. У вас может получиться что-то другое. Теоретиченски, функция может не всегда отрабатывать: она может, скажем, заменить символ на точно такой же или заменить два соседних символа, которые в исконном слове одинаковы, или при выборе параметра warps > 1, функция может заменить дважды один и тот же символ и так далее. \n",
    "\n",
    "Это можно было бы предусмотреть, но мы посчитали это нецелесообразным, так как скорее всего функция создаст опечатку и мы получим достаточное количество новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаём обучающую выборку\n",
    "\n",
    "Теперь, собственно, создаём саму обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28030\n",
      "28030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternate_name</th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>0</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Андорра</td>\n",
       "      <td>0</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>-290557</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Объединенные Арабские Эмираты</td>\n",
       "      <td>-290557</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Арабские Эмираты</td>\n",
       "      <td>-290557</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  alternate_name  geoname_id                  name\n",
       "0                        Andorra           0               Andorra\n",
       "1                        Андорра           0               Andorra\n",
       "2           United Arab Emirates     -290557  United Arab Emirates\n",
       "3  Объединенные Арабские Эмираты     -290557  United Arab Emirates\n",
       "4               Арабские Эмираты     -290557  United Arab Emirates"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Убираем из датафрейма записи, содержащие ссылки на ресурсы в интернете\n",
    "df_alter = df_alter[~df_alter[ALTERNATE_NAME_COLUMN].str.contains(\"https://\")]\n",
    "\n",
    "# Объединяем датафреймы df_alter и df_cities_emb по geoname_id так, чтобы в одной колонке были значения из df_alter, а вдругой из \n",
    "#df_cities_emb с соответствующем geoname_id\n",
    "\n",
    "df_learn = df_alter.merge(df_cities_emb, on = GEONAME_ID_COLUMN).copy()\n",
    "\n",
    "# Проверяем что всё впорядке\n",
    "print(len(df_learn)), print(len(df_alter))\n",
    "df_learn.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что всё хорошо - длина df_alter и df_learn совпадают, как и должно быть в поле alternate_name альтернативные названия, а в поле name - унифицированные. Теперь нам надо применить к обучающей выборке функции, которые унифицируют слова и добавят аугментированные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b5d20059444571a6026ca521202183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Унифицируем слова в колонках с истинным и альтернативным именами приводя их к виду (Москва = moskva). \n",
    "df_learn[ALTERNATE_NAME_COLUMN] = df_learn[ALTERNATE_NAME_COLUMN].apply(prepare_word) \n",
    "df_learn[NAME_COLUMN] = df_learn[NAME_COLUMN].apply(prepare_word) \n",
    "\n",
    "df_alter = df_learn.copy()\n",
    "# Теперь мы будем применять к столбцу с альтернативными названиями функцию, добавляющую опечатки, \n",
    "# а затем будем добавлять получившийся результат в обучающий датасет. Если значение WARPS равно нулю, то опечатки добавлены не будут,\n",
    "# но программа отработает корректно\n",
    "for i in tqdm(range(WARPS)):\n",
    "   \n",
    "# Нам нужен отдельный датафрейм, чтобы применять к нему опечатки и затем соединять с обучающим, иначе каждая последующая итерация будет\n",
    "# применять опечатки и к тому что добавлено, удваивая датасет\n",
    "    df_learn_warped = df_alter.copy()\n",
    "# Применяем опечатки\n",
    "    df_learn_warped[ALTERNATE_NAME_COLUMN] = df_learn_warped[ALTERNATE_NAME_COLUMN].apply(lambda x: warp(word = x, warps = 1))\n",
    "# Присоединяем результат к обучающей выборке\n",
    "    df_learn = pd.concat([df_learn,df_learn_warped])\n",
    "\n",
    "# При добавлении опечаток больше одного раза могут образоваться дубликаты, так что выкидываем их\n",
    "df_learn = df_learn.drop_duplicates(subset = ALTERNATE_NAME_COLUMN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternate_name</th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23722</th>\n",
       "      <td>mehzgore</td>\n",
       "      <td>12041452</td>\n",
       "      <td>mezgore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23722</th>\n",
       "      <td>mezhgoer</td>\n",
       "      <td>12041452</td>\n",
       "      <td>mezgore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23722</th>\n",
       "      <td>mezhgore</td>\n",
       "      <td>12041452</td>\n",
       "      <td>mezgore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23458</th>\n",
       "      <td>fedorovskii</td>\n",
       "      <td>11886891</td>\n",
       "      <td>fedorovskiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23458</th>\n",
       "      <td>fedorovkii</td>\n",
       "      <td>11886891</td>\n",
       "      <td>fedorovskiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22903</th>\n",
       "      <td>zerzhinskii</td>\n",
       "      <td>8521440</td>\n",
       "      <td>dzerzhinsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22904</th>\n",
       "      <td>q1n35189</td>\n",
       "      <td>8521440</td>\n",
       "      <td>dzerzhinsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22904</th>\n",
       "      <td>q135n189</td>\n",
       "      <td>8521440</td>\n",
       "      <td>dzerzhinsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22903</th>\n",
       "      <td>dgzerzhinskii</td>\n",
       "      <td>8521440</td>\n",
       "      <td>dzerzhinsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22904</th>\n",
       "      <td>q135189</td>\n",
       "      <td>8521440</td>\n",
       "      <td>dzerzhinsky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alternate_name  geoname_id         name\n",
       "23722       mehzgore    12041452      mezgore\n",
       "23722       mezhgoer    12041452      mezgore\n",
       "23722       mezhgore    12041452      mezgore\n",
       "23458    fedorovskii    11886891  fedorovskiy\n",
       "23458     fedorovkii    11886891  fedorovskiy\n",
       "22903    zerzhinskii     8521440  dzerzhinsky\n",
       "22904       q1n35189     8521440  dzerzhinsky\n",
       "22904       q135n189     8521440  dzerzhinsky\n",
       "22903  dgzerzhinskii     8521440  dzerzhinsky\n",
       "22904        q135189     8521440  dzerzhinsky"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем результат\n",
    "df_learn.sort_values(by = GEONAME_ID_COLUMN, ascending = False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, опечатки были созданы.\n",
    "\n",
    "## Обучение модели\n",
    "\n",
    "Напишем функцию для обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19048 rows × 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0415dcf7991b4324b7c0475547ccfdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd34bf7422054e97a8b8c724b50835f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26974f04bb645998217245bf9e31439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def fit_model(fit_df = df_learn):\n",
    "    # Подгружаем модель, которую будем обучать\n",
    "    model = SentenceTransformer(BASE_MODEL)\n",
    "    # Формируем списки слов из колонки с истинными и альтернативными именами\n",
    "    sentences1 = fit_df[NAME_COLUMN].to_list()\n",
    "    sentences2 = fit_df[ALTERNATE_NAME_COLUMN].to_list()\n",
    "    # Создаём пустой список для последующего хранения в нём InputExample\n",
    "    train_examples = []\n",
    "\n",
    "    for i in tqdm(range(len(sentences1))):\n",
    "    # Заполняем список InputExample. Это особый тип сущностей, которые понимают подели, по сути две строки, которые являются обучабщим примером\n",
    "        train_examples.append(InputExample(texts=[sentences1[i], sentences2[i]]))\n",
    "    \n",
    "    # Инициализируем DataLoader, который запихнём в модель наши данные. Размер батча равен нашей константе.\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    # Функция потерь, от которой зависит алгоритм обучения модели. Конкретная функция потерь позволяет обучать модель без \n",
    "    # меток класса соответствия\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "    # Собственно, обучаем модель и сохраняем её. Путь указан в константе FINAL_MODEL.\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=EPOCHS,  \n",
    "               output_path=FINAL_MODEL)\n",
    "\n",
    "# Обучаем модель, если указано, что её нужно обучать.\n",
    "if FIT_MODEL == 0:\n",
    "    fit_model()\n",
    "else:\n",
    "    print('Модель уже обучена')\n",
    "\n",
    "# Загружаем итоговую модель в любом случае (если вы не обучали, то загружается сохранённая). Если вы брали модель \"из коробки\", а потом всё же\n",
    "# решили дообучить её, не забудьте поменять путь.\n",
    "model = SentenceTransformer(FINAL_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текста\n",
    "\n",
    "Теперь нам нужно векторизовать тексты из той выборки, где будем искать текст, то есть, создать эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(df_emb = df_cities_emb, column_name = NAME_COLUMN):\n",
    "# Так как процесс долгий, он специально разбит на этапы, чтобы был виден прогресс. Для этого разделим длину датасета на 100 с округлением вверх\n",
    "# и будем по частям сохранять векторы.\n",
    "    n = int(-1 * len(df_emb) // 100 * -1)\n",
    "# Также не забудем применить prepare_word к нашим данным. \n",
    "    df_emb[column_name] = df_emb[column_name].apply(prepare_word)\n",
    "    for i in tqdm(range(n)):\n",
    "# Копируем наш датасет, чтобы работать с копией\n",
    "        df2 = df_emb.copy()\n",
    "# Получаем корпус текстов в текущем срезе данных, который зависит от этапа\n",
    "        corpus = list(df2.iloc[i*100:(i+1)*100][column_name])\n",
    "# Кодируем текст с помощью модели\n",
    "        embeddings = model.encode(corpus)\n",
    "# Преобразуем результат в датафрейм. Затем по индексу соединяем его с соответсвующей частью исходных данных\n",
    "        df_return=pd.DataFrame(data=embeddings) \n",
    "        df_return = pd.merge(df2.iloc[i*100:(i+1)*100].reset_index(drop = True), df_return, left_index=True, right_index=True)\n",
    "# В случае первой итерации создаём результирующий датафрейм, если итерация не первая - присоединяем результат к результирующему датафрейму\n",
    "        if i == 0:\n",
    "            df_result = df_return\n",
    "        else:\n",
    "            df_result= pd.concat([df_result,df_return])\n",
    "    return df_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги довольно тяжёлые данные. Нам нужно сохранить их в базу данных, будем делать это по частям (по 1000 записей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(df_emb = df_cities_emb, column_name = NAME_COLUMN):\n",
    "# Вызываем предыдущую функцию, чтобы получить эмбеддинги, которые будем сохранять.\n",
    "    df_emb = make_embeddings(df_emb, column_name)\n",
    "# Делим данные на 1000 с округлением вверх\n",
    "    n = int(-1 * len(df_emb) // 1000 * -1)\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "# Берём срез данных\n",
    "        df2 = df_emb.iloc[i*1000:(i+1)*1000]\n",
    "# Если это первая итерация, то перезаписываем таблицу, если нет, то добавляем к уже существующей\n",
    "        if i == 0:\n",
    "            df2.to_sql(EMBEDDINGS, engine, if_exists='replace', index=False)\n",
    "        else: \n",
    "            df2.to_sql(EMBEDDINGS, engine, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d9c18935cb4e078f0cb8b0dda3ce18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f077c052094c41f4bb0a94aa63bb4233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица с опциями обновлена\n"
     ]
    }
   ],
   "source": [
    "# Применяем функцию если мы указали соотвествующие опции\n",
    "if VECTORIZATION == 0:\n",
    "    save_embeddings(df_emb = df_cities_emb)\n",
    "else:\n",
    "    print('Эмбеддинги уже созданы')\n",
    "\n",
    "# Если мы хотим автоматически обновлять опции, то делаем это. Мы просто присвоим значения 1 в оба столбца, так как при любом раскладе\n",
    "# это и должно случиться. \n",
    "if AUTO_UPDATE_OPTIONS:\n",
    "    update_database = pd.DataFrame( columns = ['fit_model', 'vectorization'])\n",
    "    update_database.loc[0] = [1, 1]\n",
    "    update_database.to_sql(OPTIONS, engine, if_exists='replace', index=False)\n",
    "    print('Таблица с опциями обновлена')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги созданы. Теперь нужно использовать их для предсказания.\n",
    "\n",
    "## Предсказание. Тестирование модели\n",
    "\n",
    "Для начала загрузим наши эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings():\n",
    "# Считываем эмбеддинги из базы данных.\n",
    "    query = f'SELECT *  FROM \"{EMBEDDINGS}\"'\n",
    "    df_embeddings= pd.read_sql_query(query, con=engine)\n",
    "\n",
    "# Преобразуем их в np.Array чтобы модель могла с ними работать. Для начала возьмём из датафрейма только те столбцы, \n",
    "# где записаны получившиеся числа, затем преобразуем их в np.array типа float \n",
    "    embeddings = df_embeddings.iloc[:, 2:].values.tolist()\n",
    "    embeddings = np.array(embeddings,dtype='float32')\n",
    "    return df_embeddings, embeddings\n",
    "\n",
    "df_embeddings, embeddings = read_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция предсказания\n",
    "\n",
    "По условию задачи, мы хотим возвращать список словарей, в которых будут следующие ключи с соответствующими значениями: <code>geonameid, name, region, country, cosine_similarity</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>geoname_id</th>\n",
       "      <th>name</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Netherlands Antilles</td>\n",
       "      <td>-8505032.0</td>\n",
       "      <td>Netherlands Antilles</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>South Sudan</td>\n",
       "      <td>-7909807.0</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bonaire, Saint Eustatius and Saba</td>\n",
       "      <td>-7626844.0</td>\n",
       "      <td>Bonaire, Saint Eustatius and Saba</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                country  geoname_id  \\\n",
       "259                Netherlands Antilles  -8505032.0   \n",
       "203                         South Sudan  -7909807.0   \n",
       "29   Bonaire, Saint Eustatius and Saba   -7626844.0   \n",
       "\n",
       "                                   name region  \n",
       "259                Netherlands Antilles    NOT  \n",
       "203                         South Sudan    NOT  \n",
       "29   Bonaire, Saint Eustatius and Saba     NOT  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_df():\n",
    "    # Запрашиваем заново нужные колонки из таблицы с городами\n",
    "    query = f'SELECT \"{GEONAME_ID_COLUMN}\",\"{NAME_COLUMN}\",\"{ADMIN_CODES_COLUMN}\", \"{COUNTRY_CODE_COLUMN}\"  FROM {CITIES}'\n",
    "    result_cities = pd.read_sql_query(query, con=engine)\n",
    "    \n",
    "    #Для получения стран с заполненными geoname_id запрашиваем функцию, которую мы уже писали, когда генерировали заполненные пропуски\n",
    "    result_countries = make_countries(save_country_code=True)\n",
    "    \n",
    "    # Переименуем колонку iso в country_code\n",
    "    result_countries = result_countries.rename(columns = {ISO_COLUMN:COUNTRY_CODE_COLUMN})\n",
    "\n",
    "    # Нам надо получить страну вместо кода страны. Смержим нашу таблицу с городами с таблицей со странами по колоке с кодами страны\n",
    "    # Так же, уберём geoname_id из второго датасета, так как нас интересует только то, чтобы появилась релевантная колонка со странами\n",
    "    result = result_cities.merge(result_countries.drop(columns=[GEONAME_ID_COLUMN]), on = COUNTRY_CODE_COLUMN)\n",
    "\n",
    "    # Добавим в датафрейм со странами колонку name, которая соответсвует стране\n",
    "    result_countries[NAME_COLUMN] = result_countries[COUNTRY_COLUMN]\n",
    "\n",
    "    # Объединим наш датафрейм со странами методом конкат, чтобы добавить сами страны\n",
    "    result = pd.concat([result_countries, result]).fillna('NONE')\n",
    "    \n",
    "    # Теперь нам нужно получить регион. В таблице с регионами его код представлен в формате AD.05. Т.е. нам придётся соединить строку\n",
    "    # с кодом страны и admin1_code, а затем смержить наш датасет с датасетом с регионами по этому столбцу\n",
    "    result[ADMIN_CODES_COLUMN] = result[COUNTRY_CODE_COLUMN]  +'.'+ result[ADMIN_CODES_COLUMN]\n",
    "    # Переименуем получившийся столбец\n",
    "    result = result.rename(columns = {ADMIN_CODES_COLUMN:CODE_COLUMN})\n",
    "    \n",
    "    # Подгрузим данные с регионами\n",
    "    query = f'SELECT \"{CODE_COLUMN}\", \"{REGION_NAME_ASCII_COLUMN}\" FROM \"{ADMIN_CODES}\"'\n",
    "    result_admin = pd.read_sql_query(query, con=engine)\n",
    "\n",
    "    # Смержим с нашим датасетом\n",
    "    result = result.merge(result_admin, on = CODE_COLUMN, how = 'outer')\n",
    "    \n",
    "    # В датасете есть куча регионов которые не встречаются у нас, потому появились пропуски в интересующих нас колонках. Дропнем их\n",
    "    result.dropna(subset=GEONAME_ID_COLUMN, inplace=True)\n",
    "\n",
    "    # Дропнем колонки code и country_code, т.к. они не нужна\n",
    "    result.drop(columns = [CODE_COLUMN], inplace=True)\n",
    "    result.drop(columns = [COUNTRY_CODE_COLUMN], inplace=True)\n",
    "\n",
    "    # Переименуем name_ascii. В новом имени не нужна константа, так этот датасет генерируется нами\n",
    "    result = result.rename(columns = {REGION_NAME_ASCII_COLUMN: 'region'})\n",
    "    result.fillna('NOT', inplace=True)\n",
    "\n",
    "    return result.sort_values(by=GEONAME_ID_COLUMN)\n",
    "\n",
    "search_df().head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы получили датафрейм, в котором есть все нужные данные, кроме косинусного расстояния. Его мы получим только в функции предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'geoname_id': 2013348,\n",
       "  'cosine_similarity': 0.7519763112068176,\n",
       "  'country': 'Russia',\n",
       "  'name': 'Vladivostok',\n",
       "  'region': 'Primorye'},\n",
       " {'geoname_id': 472459,\n",
       "  'cosine_similarity': 0.6214302778244019,\n",
       "  'country': 'Russia',\n",
       "  'name': 'Vologda',\n",
       "  'region': 'Vologda Oblast'}]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_cities_df = search_df()\n",
    "\n",
    "def predict(\n",
    "        # Слово, которое мы будем искать\n",
    "        word, \n",
    "        # Количество сходств, которое мы будем искать\n",
    "        top_k = 5,\n",
    "        # Датафрейм из которого будут извлекаться словари\n",
    "        search_cities_df = search_cities_df,\n",
    "        # Считанный из базы данных датафрейм с эмбеддингами\n",
    "        df_embeddings = df_embeddings,\n",
    "        # Вектор с эмбеддингами\n",
    "        embeddings = embeddings,\n",
    "        # Будем ли мы возвращать предсказания в виде словаря, как указано в техническом задании или в виде датафрейма?\n",
    "        # (Естественно, по умолчанию в виде словаря)\n",
    "        to_dict = True\n",
    "        \n",
    "        ):\n",
    "    # Обработаем входящее слово\n",
    "    word = prepare_word(word)\n",
    "    # С помощью модели рассчитаем ветор для нашего входящего слова\n",
    "    vector = model.encode([word])\n",
    "    # Получаем результат нашего предсказания в виде индексов и сходств \n",
    "    result = semantic_search(vector,embeddings,top_k = top_k)\n",
    "    # Инициализируем списки для id наших корпусов и косинусных расстояний\n",
    "    cos_sims = []\n",
    "    indexes = []\n",
    "    # Запишем их в списки\n",
    "    for i in result[0][:]:\n",
    "        indexes.append(i['corpus_id'])\n",
    "        cos_sims.append(i['score'])\n",
    "    \n",
    "    # Ищем geoname_id в датафрейме с эмбеддингами, который мы загрузили из базы данных\n",
    "    pred_df = df_embeddings.iloc[indexes].copy()[GEONAME_ID_COLUMN].to_frame()\n",
    "    # Добавляем косинусное сходство\n",
    "    pred_df['cosine_similarity'] = cos_sims\n",
    "    # Мержим с датафреймом, который мы специально готовили выше\n",
    "    pred_df = pred_df.merge(search_cities_df, on = GEONAME_ID_COLUMN)\n",
    "    \n",
    "    if to_dict:\n",
    "        return pred_df.to_dict('records') \n",
    "    else:\n",
    "        return pred_df\n",
    "\n",
    "predict('Влодивасток', top_k = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша функция работает как было указано"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование\n",
    "\n",
    "Теперь протестируем нашу модель. Заказчик предоставил нам тестовый датасет. Посмотрим на него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>name</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Смоленск</td>\n",
       "      <td>Smolensk</td>\n",
       "      <td>Smolensk Oblast</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Кемерово</td>\n",
       "      <td>Kemerovo</td>\n",
       "      <td>Kuzbass</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Бишкек</td>\n",
       "      <td>Bishkek</td>\n",
       "      <td>Bishkek</td>\n",
       "      <td>Kyrgyzstan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Москва</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Алматы</td>\n",
       "      <td>Almaty</td>\n",
       "      <td>Almaty</td>\n",
       "      <td>Kazakhstan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      query      name           region     country\n",
       "0  Смоленск  Smolensk  Smolensk Oblast      Russia\n",
       "1  Кемерово  Kemerovo          Kuzbass      Russia\n",
       "2    Бишкек   Bishkek          Bishkek  Kyrgyzstan\n",
       "3    Москва    Moscow           Moscow      Russia\n",
       "4    Алматы    Almaty           Almaty  Kazakhstan"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(TEST_CSV, sep = ';')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересуют поля query, которые содержат запрос и поле name, которое содержит истинное название города. Мы будем делать предсказания по всей тестовой выборке и сравнивать результаты с истинным именем, указанным в текстовом датасете. В качестве результата мы будем возвращать две метрики:\n",
    "- <b>accuracy</b> - количество совпадений названий из первого места списка.\n",
    "- <b>accuracy*k</b> - количество попаданий правильного названия в любое место списка длиной k. В данном случае для k возьмём значение 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4218ec8e4fc249d3b276ee370e78dc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test(\n",
    "        # Название тестового csv (в константе)\n",
    "        test_csv = TEST_CSV, \n",
    "        # Разделитель (там точка с запятой)\n",
    "        sep = ';',  \n",
    "        # Длина списка предсказаний\n",
    "        k = 5):\n",
    "    \n",
    "    # Считываем тестовый csv\n",
    "    test_df = pd.read_csv(test_csv, sep = sep)\n",
    "    # Конвертируем запросы в список\n",
    "    querys = test_df['query'].to_list()\n",
    "    # Наши метрики, о которых было написано выше. Пока они равны нулю\n",
    "    accuracy = 0\n",
    "    accuracy_k = 0\n",
    "    for i in tqdm(range(len(querys))):\n",
    "        # Получаем предсказание. to_dict ставим False, так как тут удобней работать с датафреймом\n",
    "        predicted_name = predict(querys[i], top_k = k, to_dict = False).reset_index(drop = True)\n",
    "        # Сохраняем полученный датафрейм значение в новую переменную\n",
    "        predicted_name_k = predicted_name\n",
    "        # В predicted_name сохраняем только предсказанное имя на первой строке\n",
    "        predicted_name = predicted_name.loc[0][NAME_COLUMN]\n",
    "        # Забираем истинное название города и преобразуем его в унифицированную форму\n",
    "        true_name = prepare_word(test_df.loc[i]['name'])\n",
    "        \n",
    "        # Сравниваем его с первым предсказанным именем. Если они равны, то даём очко accuracy\n",
    "        if true_name == prepare_word(predicted_name):\n",
    "            accuracy += 1\n",
    "        \n",
    "        # Ищем истинное имя в списке предсказанных имён на любой позиции (список предсказанных имён ограничен).\n",
    "        # Если находим то даём очко accuracy_k\n",
    "        # Заметим, accuracy_k не может быть меньше accuracy, т.к. если верное предсказание на первом месте, то accuracy_k тоже засчитается\n",
    "        if true_name in predicted_name_k[NAME_COLUMN].apply(prepare_word).to_list():\n",
    "            accuracy_k += 1\n",
    "\n",
    "    # Делим полученные значения на длину выборки.\n",
    "    accuracy = accuracy / len(test_df)\n",
    "    accuracy_k = accuracy_k / len(test_df)\n",
    "    return accuracy, accuracy_k\n",
    "\n",
    "accuracy, accuracy_k = test(test_csv = TEST_CSV, sep = ';', k = TEST_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение accuracy = 0.925, значение accuracy*5 = 0.939\n"
     ]
    }
   ],
   "source": [
    "print(f'Значение accuracy = {accuracy:,.3f}, значение accuracy*{TEST_K} = {accuracy_k:,.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы протестировали нашу модель, она показала значения accuracy = 0.925, значение accuracy*5 = 0.939 на тестовой выборке. \n",
    "\n",
    "## Вывод\n",
    "\n",
    "В ходе исследования были испробованы такие подходы, как TfidfVectorizer и написание собственной нейронной сети, но в конце концов было решено остановиться на предобученной нейронной сети Sentence Transformers. Конкретно за основу нашей модели была взята distiluse-base-multilingual-cased-v2 и дообучена на альтернативных названиях городов, при том сам поиск производится только на унифицированным названиям городов. Был испробован альтернативный вариант - искать не только по унифицированным названиям городов, но и по альтернативным, ведь мы знаем их geoname_id и можем сопоставить с унифицированными названиями. Однако, такой подход имеет существенный недостаток - поиск происходит довольно медленно, что неприемлемо для целей заказчика.\n",
    "\n",
    "Так же, исследования показали, что предварительная обработка слов и добавление в обучающую выборку аугментированных данных очень существенно улучшают результаты работы модели. \n",
    "\n",
    "#### Были выполнены следующие задачи\n",
    "\n",
    "- Мы настроили подключение к базе данных. Таким образом, программа работает непосредственно с базой данных заказчика.\n",
    "- Были созданы настройки, благодаря которым заказчик сможет использовать любые данные схожей структуры. Он сможет использовать свои названия для схем данных и колонок, таким образом ему не придётся вмешиваться в код программы, чтобы подстроить её под свои данные или же менять названия своих данных (но важно чтобы структура данных сохранялась)\n",
    "- Программа записывает в базу данных заказчика готовые эмбеддинги, таким образом, ему не придётся каждый раз заново рассчитывать их.\n",
    "- Был создан скрипт, который создаёт и считывает дополнительную схему данных, в которой указывается, требуется ли повторное обучение модели и создание эмбеддингов. Более того, программа может считать эти настройки из базы данных, таким образом не обязательно каждый раз менять значения соответствующих констант.\n",
    "- Была самостоятельно создана таблица с дополнительными альтернативными названиями для стран.\n",
    "- Были созданы настройки обучения модели. Таким образом, заказчик может поменять параметры обучения просто поменяв значения некоторых констант.\n",
    "- Был создан (также настраиваемый) скрипт для предподготовки данных (который включает в себя и аугментацию)\n",
    "- Была обучена модель, основанная на  distiluse-base-multilingual-cased-v2. Обучение длилось 4 эпохи. Модель показала результаты accuracy = 0.925, значение accuracy*5 = 0.939. Она умеет справляться с опечатками, как было сказано в ТЗ.\n",
    "- Была создана финальная функция predict(), которая принимает слово и возвращает список словарей, соответсвующий техническому задания. При том, в функции есть настройка, позволяющая выдавать вместо списка словарей датафрейм, что может быть удобней для тестирования непосредственно в теле программы. Также, согласно ТЗ, в функцию добавлен параметр, определяющий длину возвращаемого списка.\n",
    "- Модель также умеет определять страны, не только города.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что может улучшить модель?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Со стороны заказчика</b>. Согласно заданию, модель должна помогать найти унифицированное название города среди вакансий, но сам поиск проводится вручную, программа лишь помогает тем, что выдаёт список из возможных ближайших городов. Соответственно, можно настроить скрипт, который будет автоматически заносить в некую новую таблицу данных все пользовательские названия городов и соответствующие ему унифицированные названия городов. Затем, раз в какое-то время сверять её с базой с альтернативными городами и добавлять туда все несовпадающие названия, таким образом можно постепенно расширять базу альтернативных городов и соответственно расширять обучающую выборку.\n",
    "\n",
    "<b>Со стороны разработчика</b>. Качество модели можно улучшать бесконечно, так как нет предела совершенству, но вот несколько нереализованных пока идей:\n",
    "- Найти более подходящий способ унифицировать слова. Возможно, использовать более продвинутые библиотеки для перевода и транслитерации (такие библиотеки предлагались, но они иногда вызывали ошибки, так как не всегда могли определить язык)\n",
    "- Добавить в поисковую выборку некоторое количество альтернативных названий, например таких, с которыми модель видит наименьшее сходство. Таким образом, добиться некоторого компромисса - и расширить поисковую выборку, и не слишком потерять в скорости.\n",
    "- Подробно проанализировать ошибки, понять слабые места модели, возможно, самостоятельно дополнить обучающую выборку.\n",
    "- Перепробовать больше моделей с разными параметрами."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
